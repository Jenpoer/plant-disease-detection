{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfd7085",
   "metadata": {},
   "source": [
    "# CCT model\n",
    "\n",
    "## Overview\n",
    "\n",
    "* Using a pretrained model that is from this repo: https://github.com/SHI-Labs/Compact-Transformers \n",
    "* Copied the cct main python file and the utils because it is not in a form of package that we can just import \n",
    "* Curretntly using cct model with pretrained imagenet\n",
    "\n",
    "## Model choice\n",
    "* cct_14_7x2_224\n",
    "* cct_14_7x2_384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb121c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.24-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torchvision in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: pandas in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pillow in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (12.1.0)\n",
      "Requirement already satisfied: torch in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from timm) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from timm) (6.0.3)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torchvision) (2.4.2)\n",
      "Requirement already satisfied: filelock in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from torch->timm) (2026.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub->timm)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface_hub->timm)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Collecting shellingham (from huggingface_hub->timm)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from huggingface_hub->timm) (4.67.3)\n",
      "Collecting typer-slim (from huggingface_hub->timm)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface_hub->timm)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: colorama in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\Users\\jessl\\SMU (Master)\\computer_vision\\.venv\\Lib\\site-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
      "Downloading timm-1.0.24-py3-none-any.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 6.4 MB/s  0:00:00\n",
      "Downloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "   ---------------------------------------- 0.0/553.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 553.3/553.3 kB 18.2 MB/s  0:00:00\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.8/2.9 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 4.8 MB/s  0:00:00\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: shellingham, safetensors, httpcore, hf-xet, typer-slim, httpx, huggingface_hub, timm\n",
      "\n",
      "   ---------------------------------------- 0/8 [shellingham]\n",
      "   ----- ---------------------------------- 1/8 [safetensors]\n",
      "   ----- ---------------------------------- 1/8 [safetensors]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   ---------- ----------------------------- 2/8 [httpcore]\n",
      "   --------------- ------------------------ 3/8 [hf-xet]\n",
      "   -------------------- ------------------- 4/8 [typer-slim]\n",
      "   -------------------- ------------------- 4/8 [typer-slim]\n",
      "   -------------------- ------------------- 4/8 [typer-slim]\n",
      "   -------------------- ------------------- 4/8 [typer-slim]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------- -------------- 5/8 [httpx]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ------------------------------ --------- 6/8 [huggingface_hub]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ----------------------------------- ---- 7/8 [timm]\n",
      "   ---------------------------------------- 8/8 [timm]\n",
      "\n",
      "Successfully installed hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-1.4.1 safetensors-0.7.0 shellingham-1.5.4 timm-1.0.24 typer-slim-0.21.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 26.0 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install timm torchvision pandas pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924312de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62aa82b",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad97ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, df, repo_root, img_size=224, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.repo_root = Path(repo_root)\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = self.repo_root / row[\"filepath_rel\"]\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        label = int(row[\"canonical_id\"])\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a61835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([ 2, 25, 14, 25, 14,  1, 22, 25, 18, 25])\n"
     ]
    }
   ],
   "source": [
    "repo_root = Path.cwd().parent\n",
    "splits_dir = repo_root / \"data\" / \"splits\"\n",
    "\n",
    "\n",
    "train_csv = pd.read_csv(splits_dir / \"pv_train.csv\")\n",
    "val_csv   = pd.read_csv(splits_dir / \"pv_val.csv\")\n",
    "test_csv  = pd.read_csv(splits_dir / \"pv_test.csv\")  \n",
    "\n",
    "train_dataset = PlantDataset(train_csv, repo_root)\n",
    "val_dataset   = PlantDataset(val_csv, repo_root)\n",
    "test_dataset  = PlantDataset(test_csv, repo_root)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Quick sanity check\n",
    "imgs, labels = next(iter(train_loader))\n",
    "print(imgs.shape) \n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f954dd5f",
   "metadata": {},
   "source": [
    "# setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import optim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad4bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing classifier.fc.weight, number of classes has changed.\n",
      "Removing classifier.fc.bias, number of classes has changed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.conv_layers.0.0.weight\n",
      "tokenizer.conv_layers.1.0.weight\n",
      "classifier.positional_emb\n",
      "classifier.attention_pool.weight\n",
      "classifier.attention_pool.bias\n",
      "classifier.blocks.0.pre_norm.weight\n",
      "classifier.blocks.0.pre_norm.bias\n",
      "classifier.blocks.0.self_attn.qkv.weight\n",
      "classifier.blocks.0.self_attn.proj.weight\n",
      "classifier.blocks.0.self_attn.proj.bias\n",
      "classifier.blocks.0.linear1.weight\n",
      "classifier.blocks.0.linear1.bias\n",
      "classifier.blocks.0.norm1.weight\n",
      "classifier.blocks.0.norm1.bias\n",
      "classifier.blocks.0.linear2.weight\n",
      "classifier.blocks.0.linear2.bias\n",
      "classifier.blocks.1.pre_norm.weight\n",
      "classifier.blocks.1.pre_norm.bias\n",
      "classifier.blocks.1.self_attn.qkv.weight\n",
      "classifier.blocks.1.self_attn.proj.weight\n",
      "classifier.blocks.1.self_attn.proj.bias\n",
      "classifier.blocks.1.linear1.weight\n",
      "classifier.blocks.1.linear1.bias\n",
      "classifier.blocks.1.norm1.weight\n",
      "classifier.blocks.1.norm1.bias\n",
      "classifier.blocks.1.linear2.weight\n",
      "classifier.blocks.1.linear2.bias\n",
      "classifier.blocks.2.pre_norm.weight\n",
      "classifier.blocks.2.pre_norm.bias\n",
      "classifier.blocks.2.self_attn.qkv.weight\n",
      "classifier.blocks.2.self_attn.proj.weight\n",
      "classifier.blocks.2.self_attn.proj.bias\n",
      "classifier.blocks.2.linear1.weight\n",
      "classifier.blocks.2.linear1.bias\n",
      "classifier.blocks.2.norm1.weight\n",
      "classifier.blocks.2.norm1.bias\n",
      "classifier.blocks.2.linear2.weight\n",
      "classifier.blocks.2.linear2.bias\n",
      "classifier.blocks.3.pre_norm.weight\n",
      "classifier.blocks.3.pre_norm.bias\n",
      "classifier.blocks.3.self_attn.qkv.weight\n",
      "classifier.blocks.3.self_attn.proj.weight\n",
      "classifier.blocks.3.self_attn.proj.bias\n",
      "classifier.blocks.3.linear1.weight\n",
      "classifier.blocks.3.linear1.bias\n",
      "classifier.blocks.3.norm1.weight\n",
      "classifier.blocks.3.norm1.bias\n",
      "classifier.blocks.3.linear2.weight\n",
      "classifier.blocks.3.linear2.bias\n",
      "classifier.blocks.4.pre_norm.weight\n",
      "classifier.blocks.4.pre_norm.bias\n",
      "classifier.blocks.4.self_attn.qkv.weight\n",
      "classifier.blocks.4.self_attn.proj.weight\n",
      "classifier.blocks.4.self_attn.proj.bias\n",
      "classifier.blocks.4.linear1.weight\n",
      "classifier.blocks.4.linear1.bias\n",
      "classifier.blocks.4.norm1.weight\n",
      "classifier.blocks.4.norm1.bias\n",
      "classifier.blocks.4.linear2.weight\n",
      "classifier.blocks.4.linear2.bias\n",
      "classifier.blocks.5.pre_norm.weight\n",
      "classifier.blocks.5.pre_norm.bias\n",
      "classifier.blocks.5.self_attn.qkv.weight\n",
      "classifier.blocks.5.self_attn.proj.weight\n",
      "classifier.blocks.5.self_attn.proj.bias\n",
      "classifier.blocks.5.linear1.weight\n",
      "classifier.blocks.5.linear1.bias\n",
      "classifier.blocks.5.norm1.weight\n",
      "classifier.blocks.5.norm1.bias\n",
      "classifier.blocks.5.linear2.weight\n",
      "classifier.blocks.5.linear2.bias\n",
      "classifier.blocks.6.pre_norm.weight\n",
      "classifier.blocks.6.pre_norm.bias\n",
      "classifier.blocks.6.self_attn.qkv.weight\n",
      "classifier.blocks.6.self_attn.proj.weight\n",
      "classifier.blocks.6.self_attn.proj.bias\n",
      "classifier.blocks.6.linear1.weight\n",
      "classifier.blocks.6.linear1.bias\n",
      "classifier.blocks.6.norm1.weight\n",
      "classifier.blocks.6.norm1.bias\n",
      "classifier.blocks.6.linear2.weight\n",
      "classifier.blocks.6.linear2.bias\n",
      "classifier.blocks.7.pre_norm.weight\n",
      "classifier.blocks.7.pre_norm.bias\n",
      "classifier.blocks.7.self_attn.qkv.weight\n",
      "classifier.blocks.7.self_attn.proj.weight\n",
      "classifier.blocks.7.self_attn.proj.bias\n",
      "classifier.blocks.7.linear1.weight\n",
      "classifier.blocks.7.linear1.bias\n",
      "classifier.blocks.7.norm1.weight\n",
      "classifier.blocks.7.norm1.bias\n",
      "classifier.blocks.7.linear2.weight\n",
      "classifier.blocks.7.linear2.bias\n",
      "classifier.blocks.8.pre_norm.weight\n",
      "classifier.blocks.8.pre_norm.bias\n",
      "classifier.blocks.8.self_attn.qkv.weight\n",
      "classifier.blocks.8.self_attn.proj.weight\n",
      "classifier.blocks.8.self_attn.proj.bias\n",
      "classifier.blocks.8.linear1.weight\n",
      "classifier.blocks.8.linear1.bias\n",
      "classifier.blocks.8.norm1.weight\n",
      "classifier.blocks.8.norm1.bias\n",
      "classifier.blocks.8.linear2.weight\n",
      "classifier.blocks.8.linear2.bias\n",
      "classifier.blocks.9.pre_norm.weight\n",
      "classifier.blocks.9.pre_norm.bias\n",
      "classifier.blocks.9.self_attn.qkv.weight\n",
      "classifier.blocks.9.self_attn.proj.weight\n",
      "classifier.blocks.9.self_attn.proj.bias\n",
      "classifier.blocks.9.linear1.weight\n",
      "classifier.blocks.9.linear1.bias\n",
      "classifier.blocks.9.norm1.weight\n",
      "classifier.blocks.9.norm1.bias\n",
      "classifier.blocks.9.linear2.weight\n",
      "classifier.blocks.9.linear2.bias\n",
      "classifier.blocks.10.pre_norm.weight\n",
      "classifier.blocks.10.pre_norm.bias\n",
      "classifier.blocks.10.self_attn.qkv.weight\n",
      "classifier.blocks.10.self_attn.proj.weight\n",
      "classifier.blocks.10.self_attn.proj.bias\n",
      "classifier.blocks.10.linear1.weight\n",
      "classifier.blocks.10.linear1.bias\n",
      "classifier.blocks.10.norm1.weight\n",
      "classifier.blocks.10.norm1.bias\n",
      "classifier.blocks.10.linear2.weight\n",
      "classifier.blocks.10.linear2.bias\n",
      "classifier.blocks.11.pre_norm.weight\n",
      "classifier.blocks.11.pre_norm.bias\n",
      "classifier.blocks.11.self_attn.qkv.weight\n",
      "classifier.blocks.11.self_attn.proj.weight\n",
      "classifier.blocks.11.self_attn.proj.bias\n",
      "classifier.blocks.11.linear1.weight\n",
      "classifier.blocks.11.linear1.bias\n",
      "classifier.blocks.11.norm1.weight\n",
      "classifier.blocks.11.norm1.bias\n",
      "classifier.blocks.11.linear2.weight\n",
      "classifier.blocks.11.linear2.bias\n",
      "classifier.blocks.12.pre_norm.weight\n",
      "classifier.blocks.12.pre_norm.bias\n",
      "classifier.blocks.12.self_attn.qkv.weight\n",
      "classifier.blocks.12.self_attn.proj.weight\n",
      "classifier.blocks.12.self_attn.proj.bias\n",
      "classifier.blocks.12.linear1.weight\n",
      "classifier.blocks.12.linear1.bias\n",
      "classifier.blocks.12.norm1.weight\n",
      "classifier.blocks.12.norm1.bias\n",
      "classifier.blocks.12.linear2.weight\n",
      "classifier.blocks.12.linear2.bias\n",
      "classifier.blocks.13.pre_norm.weight\n",
      "classifier.blocks.13.pre_norm.bias\n",
      "classifier.blocks.13.self_attn.qkv.weight\n",
      "classifier.blocks.13.self_attn.proj.weight\n",
      "classifier.blocks.13.self_attn.proj.bias\n",
      "classifier.blocks.13.linear1.weight\n",
      "classifier.blocks.13.linear1.bias\n",
      "classifier.blocks.13.norm1.weight\n",
      "classifier.blocks.13.norm1.bias\n",
      "classifier.blocks.13.linear2.weight\n",
      "classifier.blocks.13.linear2.bias\n",
      "classifier.norm.weight\n",
      "classifier.norm.bias\n",
      "classifier.fc.weight\n",
      "classifier.fc.bias\n",
      "Trainable parameters: 163\n"
     ]
    }
   ],
   "source": [
    "from cct.cct import cct_14_7x2_224 \n",
    "\n",
    "# load pretrained\n",
    "model = cct_14_7x2_224(pretrained=True, progress=True, num_classes=26)  \n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", len(trainable_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ee40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2\n"
     ]
    }
   ],
   "source": [
    "# freeze the whole layer other thant\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier.fc\" in name:  \n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "\n",
    "print(\"Trainable parameters:\", len(trainable_params))  \n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b22cce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 1.5272, Train Acc: 0.6497, Val Loss: 0.7991, Val Acc: 0.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Train Loss: 0.6507, Train Acc: 0.8735, Val Loss: 0.4743, Val Acc: 0.9004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Train Loss: 0.4421, Train Acc: 0.9086, Val Loss: 0.3493, Val Acc: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Train Loss: 0.3428, Train Acc: 0.9262, Val Loss: 0.2811, Val Acc: 0.9404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Train Loss: 0.2812, Train Acc: 0.9385, Val Loss: 0.2395, Val Acc: 0.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Train Loss: 0.2444, Train Acc: 0.9450, Val Loss: 0.2089, Val Acc: 0.9520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Train Loss: 0.2156, Train Acc: 0.9519, Val Loss: 0.1869, Val Acc: 0.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Train Loss: 0.1939, Train Acc: 0.9554, Val Loss: 0.1722, Val Acc: 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Train Loss: 0.1775, Train Acc: 0.9579, Val Loss: 0.1579, Val Acc: 0.9612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Train Loss: 0.1652, Train Acc: 0.9608, Val Loss: 0.1467, Val Acc: 0.9639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # TRAIN \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    for imgs, labels in loop:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        loop.set_postfix(train_loss=train_loss/total, train_acc=correct/total)\n",
    "\n",
    "    train_loss /= total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loop_val = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loop_val:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            loop_val.set_postfix(val_loss=val_loss/total, val_acc=correct/total)\n",
    "\n",
    "    val_loss /= total\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2294755",
   "metadata": {},
   "source": [
    "# Test model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c7dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ba44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using test dataset\n",
    "y_true_pd, y_pred_pd = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(\"PlantDoc Test Results\")\n",
    "print(\"Accuracy :\", accuracy_score(y_true_pd, y_pred_pd))\n",
    "print(\"Precision:\", precision_score(y_true_pd, y_pred_pd, average=\"macro\"))\n",
    "print(\"Recall   :\", recall_score(y_true_pd, y_pred_pd, average=\"macro\"))\n",
    "print(\"F1       :\", f1_score(y_true_pd, y_pred_pd, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b9ab83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path.cwd().parent\n",
    "splits_dir = repo_root / \"data\" / \"splits\"\n",
    "\n",
    "\n",
    "plantdoc_test_csv = pd.read_csv(splits_dir / \"plantdoc_test_mapped.csv\")  \n",
    "\n",
    "plantdoc_test_dataset = PlantDataset(plantdoc_test_csv, repo_root)\n",
    "\n",
    "plantdoc_test_loader  = DataLoader(plantdoc_test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "862af26a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m y_true_pd, y_pred_pd = evaluate_model(\u001b[43mmodel\u001b[49m, plantdoc_test_loader, device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPlantDoc Test Results\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy :\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_true_pd, y_pred_pd))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "y_true_pd, y_pred_pd = evaluate_model(model, plantdoc_test_loader, device)\n",
    "\n",
    "print(\"PlantDoc Test Results\")\n",
    "print(\"Accuracy :\", accuracy_score(y_true_pd, y_pred_pd))\n",
    "print(\"Precision:\", precision_score(y_true_pd, y_pred_pd, average=\"macro\"))\n",
    "print(\"Recall   :\", recall_score(y_true_pd, y_pred_pd, average=\"macro\"))\n",
    "print(\"F1       :\", f1_score(y_true_pd, y_pred_pd, average=\"macro\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
