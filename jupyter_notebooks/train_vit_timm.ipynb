{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b54763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeist\\Documents\\Kuliah\\MITB\\CS604\\plant-disease-detection\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import string\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import timm\n",
    "from timm.data import resolve_model_data_config, create_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c819e87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660a7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10c8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(f'{PROJECT_ROOT}/data/splits/label_space.csv')\n",
    "NUM_LABELS = len(df_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e88ad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bbc3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, split, transforms=None):\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        if split == \"train\":\n",
    "            df = pd.read_csv(f'{PROJECT_ROOT}/data/splits/pv_train.csv')\n",
    "        elif split == \"val\":\n",
    "            df = pd.read_csv(f'{PROJECT_ROOT}/data/splits/pv_val.csv')\n",
    "        elif split == \"test_pv\":\n",
    "            df = pd.read_csv(f'{PROJECT_ROOT}/data/splits/pv_test.csv')\n",
    "        elif split == \"test_pd\":\n",
    "            df = pd.read_csv(f'{PROJECT_ROOT}/data/splits/plantdoc_test_mapped.csv')\n",
    "\n",
    "        self.samples = [\n",
    "            (f\"{PROJECT_ROOT}/{row['filepath_rel']}\", row['canonical_id'])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d9253",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af412d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "c:\\Users\\jeist\\Documents\\Kuliah\\MITB\\CS604\\plant-disease-detection\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jeist\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    \"vit_base_patch16_224\",\n",
    "    pretrained=True,\n",
    "    num_classes=NUM_LABELS\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d33eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m resolve_model_data_config, create_transform\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data_config = resolve_model_data_config(\u001b[43mmodel\u001b[49m)\n\u001b[32m      5\u001b[39m train_transform = create_transform(\n\u001b[32m      6\u001b[39m     **data_config,\n\u001b[32m      7\u001b[39m     is_training=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m val_transform = create_transform(\n\u001b[32m     11\u001b[39m     **data_config,\n\u001b[32m     12\u001b[39m     is_training=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "data_config = resolve_model_data_config(model)\n",
    "\n",
    "train_transform = create_transform(\n",
    "    **data_config,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "val_transform = create_transform(\n",
    "    **data_config,\n",
    "    is_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PlantDiseaseDataset(\n",
    "    split=\"train\",\n",
    "    transforms=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = PlantDiseaseDataset(\n",
    "    split=\"val\",\n",
    "    processor=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d082b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92369324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # -------------------\n",
    "    # Training\n",
    "    # -------------------\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision forward\n",
    "        with torch.amp.autocast(device_type=DEVICE.type):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward + step\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # Validation\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "    \n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            with torch.amp.autocast(device_type=DEVICE.type):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    val_macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    val_micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Macro F1 Score: {val_macro_f1:.4f}, Micro F1 Score: {val_micro_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
